{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"0HInnO8YB6bP"},"outputs":[],"source":["#Enlazamos con google drive y obtenemos los archivos del directorio en el que estÃ¡n los textos\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive/TFG')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-49v6riRBuR-"},"outputs":[],"source":["!pip install datasets\n","!pip install transformers\n","!pip install sentencepiece\n","\n","import pandas as pd\n","from datasets import load_dataset\n","import numpy as np\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding, EarlyStoppingCallback\n","import operator\n","import sklearn.metrics as metrics\n","import torch\n","import random\n","import datetime\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXiiXCfkBDvw"},"outputs":[],"source":["dicParametersNoMetadata = {\n","    'finiteautomata/beto-sentiment-analysis': {'tam_Embeddings':512, 'tokenizer': 'finiteautomata/beto-sentiment-analysis','learning_rate':2.3e-5,'weight_decay':3.57e-3,'batch_size':8,'directory':'./BetoTwitter'},\n","    'dccuchile/bert-base-spanish-wwm-cased': {'tam_Embeddings':512, 'tokenizer':'dccuchile/bert-base-spanish-wwm-cased','learning_rate':3.89e-06,'weight_decay':5.23e-5,'batch_size':16,'directory':'./BetoNormal'},\n","    'PlanTL-GOB-ES/roberta-base-bne': {'tam_Embeddings':512, 'tokenizer':'PlanTL-GOB-ES/roberta-base-bne','learning_rate':3.14e-06,'weight_decay':1.2e-03,'batch_size':16,'directory':'./RobertaMarIA'},\n","    'pysentimiento/robertuito-base-cased': {'tam_Embeddings':128, 'tokenizer':'pysentimiento/robertuito-base-cased','learning_rate':2.72e-05,'weight_decay':1.27e-03,'batch_size':8,'directory':'./RobertaTwitter'},\n","    'bertin-project/bertin-roberta-base-spanish': {'tam_Embeddings':512, 'tokenizer':'bertin-project/bertin-roberta-base-spanish','learning_rate':7.82e-07,'weight_decay':8.7e-04,'batch_size':8,'directory':'./BertinNormal'},\n","    'maxpe/bertin-roberta-base-spanish_semeval18_emodetection': {'tam_Embeddings':512, 'tokenizer':'bertin-project/bertin-roberta-base-spanish','learning_rate':6.26e-06,'weight_decay':1e-04,'batch_size':8,'directory':'./BertinTwitter'},\n","    'cardiffnlp/twitter-xlm-roberta-base-sentiment': {'tam_Embeddings':512, 'tokenizer':'cardiffnlp/twitter-xlm-roberta-base-sentiment','learning_rate':1.11e-05,'weight_decay':3.66e-03,'batch_size':8,'directory':'./RobertaCardiff'},\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CAqGf8SXB1kw"},"outputs":[],"source":["class MyTrainer(Trainer):\n","    def log(self, logs) -> None:\n","        logs[\"learning_rate\"] = self._get_learning_rate()\n","        super().log(logs)\n","\n","class Pruebas():\n","\n","    def __init__(self,dicObject):\n","        self.dicObject = dicObject\n","\n","\n","    def predictAndTrain(self,model,dicObject):\n","        model= AutoModelForSequenceClassification.from_pretrained(model, num_labels=4, ignore_mismatched_sizes=True)\n","        model.config.id2label = {0: 'OFP', 1: 'OFG', 2: 'NO', 3: 'NOM'}\n","        model.config.label2id = {'OFP': 0, 'OFG': 1, 'NO': 2, 'NOM': 3}\n","        dataset = load_dataset('csv', data_files={'train': 'trainPreprocess.csv', 'test':'testPreprocess.csv','validation': 'validationPreprocess.csv'})\n","        #Cargamos los datos que necesitamos desde local:\n","        self.tokenizer = AutoTokenizer.from_pretrained(dicObject['tokenizer'], model_max_length=dicObject['tam_Embeddings'])\n","        #Preprocesamos todos los textos a la vez\n","        tokenized_OffendES = dataset.map(self.preprocess_text, batched=True)\n","\n","        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n","         \n","\n","        training_args = TrainingArguments(         \n","            output_dir=dicObject['directory'],                 \n","            learning_rate=dicObject['learning_rate'],    \n","            weight_decay=dicObject['weight_decay'],        \n","            num_train_epochs=5,  \n","            per_device_train_batch_size=dicObject['batch_size'],      \n","            per_device_eval_batch_size = dicObject['batch_size'],\n","            load_best_model_at_end = True,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            metric_for_best_model=\"eval_maf\",\n","            disable_tqdm=True,\n","            seed=seed\n","    )     \n","        trainer = MyTrainer(\n","            model=model,\n","            args=training_args,         \n","            train_dataset=tokenized_OffendES[\"train\"],\n","            eval_dataset=tokenized_OffendES[\"test\"],\n","            tokenizer=self.tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=self.compute_metrics,\n","            callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]  \n","    )      \n","    \n","\n","        trainer.train() \n","        predictions = trainer.predict(tokenized_OffendES['test'])\n","        predictions = tf.math.softmax(predictions.predictions, axis=-1)\n","        tags=[]\n","        for p in predictions:\n","          max_value = max(p)\n","          tags.append(np.where(p == max_value)[0][0])\n","        print(tags)\n","        df=pd.DataFrame()\n","        df['label'] = tags\n","        df.to_csv(str(dicObject['directory'].split('/')[1]+'tags.csv'), sep='\\t', index=False)\n","        self.results(tags, tokenized_OffendES['test'][\"label\"], str(dicObject['directory'].split('/')[1]))\n","\n","    def results(self,pred_labels,true_labels, nameModel):\n","        map = metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n","        mar = metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n","        maf1 = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n","        mip = metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n","        mir = metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n","        mif1 = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n","        wp = metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n","        wr = metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n","        wf1 = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n","        mse = metrics.mean_squared_error(y_true=true_labels, y_pred=pred_labels)\n","        results = {'maf': maf1, 'map': map, 'mar': mar, 'mif': mif1, 'mip': mip, 'mir': mir, 'avgf': wf1, 'avgp': wp, 'avgr': wr}\n","        df = pd.DataFrame(results, index=[0])\n","        df.to_csv(str(nameModel+' '+'results.csv'), sep='\\t', index=False)\n","\n","    def preprocess_text(self, texts):\n","        return self.tokenizer(texts[\"comment\"],truncation=True, padding='max_length',max_length=self.dicObject['tam_Embeddings'])\n","\n","    def compute_metrics(self,p):    \n","        pred, labels = p\n","        pred = np.argmax(pred, axis=1)\n","        \n","        macp = metrics.precision_score(y_true=labels, y_pred=pred, average='macro')\n","        mar = metrics.recall_score(y_true=labels, y_pred=pred, average='macro')\n","        maf1 = metrics.f1_score(y_true=labels, y_pred=pred, average='macro')\n","        mip = metrics.precision_score(y_true=labels, y_pred=pred, average='micro')\n","        mir = metrics.recall_score(y_true=labels, y_pred=pred, average='micro')\n","        mif1 = metrics.f1_score(y_true=labels, y_pred=pred, average='micro')\n","        wp = metrics.precision_score(y_true=labels, y_pred=pred, average='weighted')\n","        wr = metrics.recall_score(y_true=labels, y_pred=pred, average='weighted')\n","        wf1 = metrics.f1_score(y_true=labels, y_pred=pred, average='weighted')\n","\n","        return {'maf': maf1, 'map': macp, 'mar': mar, 'mif': mif1, 'mip': mip, 'mir': mir, 'avgf': wf1, 'avgp': wp, 'avgr': wr}\n","\n","    def entrenarModelo(self,model, dicParametersNoMetadata):\n","        self.predictAndTrain(model, dicParametersNoMetadata)\n","\n","if __name__==\"__main__\":\n","  for i in dicParametersNoMetadata:\n","    seed = 10\n","    # python \n","    random.seed(seed)\n","    # pytorch\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n","    # numpy \n","    np.random.seed(seed)\n","\n","    p = Pruebas(dicParametersNoMetadata[i])\n","    p.entrenarModelo(i,dicParametersNoMetadata[i])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hs5tnwdZ0wYC"
      },
      "outputs": [],
      "source": [
        "#Enlazamos con google drive y obtenemos los archivos del directorio en el que estÃ¡n los textos\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/TFG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E38SX3C1Bz3"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding, EarlyStoppingCallback\n",
        "import operator\n",
        "import sklearn.metrics as metrics\n",
        "import torch\n",
        "import random\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from datasets import ClassLabel, Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6ZZlMUa1EuQ"
      },
      "outputs": [],
      "source": [
        "dicParametersNoMetadata = {\n",
        "    'finiteautomata/beto-sentiment-analysis': {'tam_Embeddings':512, 'tokenizer': f'finiteautomata/beto-sentiment-analysis','learning_rate':2.3e-5,'weight_decay':3.57e-3,'batch_size':8,'directory':'./BetoTwitter'},\n",
        "    'dccuchile/bert-base-spanish-wwm-cased': {'tam_Embeddings':512, 'tokenizer':f'dccuchile/bert-base-spanish-wwm-cased','learning_rate':3.89e-06,'weight_decay':5.23e-5,'batch_size':16,'directory':'./BetoNormal'},\n",
        "    'PlanTL-GOB-ES/roberta-base-bne': {'tam_Embeddings':512, 'tokenizer':f'PlanTL-GOB-ES/roberta-base-bne','learning_rate':3.14e-06,'weight_decay':1.2e-03,'batch_size':16,'directory':'./RobertaMarIA'},\n",
        "    'cardiffnlp/twitter-xlm-roberta-base-sentiment': {'tam_Embeddings':512, 'tokenizer':f'cardiffnlp/twitter-xlm-roberta-base-sentiment','learning_rate':1.11e-05,'weight_decay':3.66e-03,'batch_size':8,'directory':'./RobertaCardiff'},\n",
        "    'maxpe/bertin-roberta-base-spanish_semeval18_emodetection': {'tam_Embeddings':512, 'tokenizer':f'bertin-project/bertin-roberta-base-spanish','learning_rate':6.26e-06,'weight_decay':1e-04,'batch_size':8,'directory':'./BertinTwitter'},\n",
        "    'bertin-project/bertin-roberta-base-spanish': {'tam_Embeddings':512, 'tokenizer':f'bertin-project/bertin-roberta-base-spanish','learning_rate':7.82e-07,'weight_decay':8.7e-04,'batch_size':8,'directory':'./BertinNormal'},    \n",
        "    'pysentimiento/robertuito-base-cased': {'tam_Embeddings':128, 'tokenizer':f'pysentimiento/robertuito-base-cased','learning_rate':2.72e-05,'weight_decay':1.27e-03,'batch_size':8,'directory':'./RobertaTwitter'},\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkuq4N1W1I3m"
      },
      "outputs": [],
      "source": [
        "input_dim = 768 + 17\n",
        "output_dim = 4\n",
        "\n",
        "class PosModelNoCapaOculta(nn.Module):\n",
        "    def __init__(self,num_labels=4, modelo=\"\"):\n",
        "        super(PosModelNoCapaOculta, self).__init__()\n",
        "        \n",
        "        self.num_labels= num_labels\n",
        "        self.modelo = modelo\n",
        "        self.base_model = AutoModel.from_pretrained(modelo,num_labels=self.num_labels)\n",
        "        self.dropout = nn.Dropout(self.base_model.config.classifier_dropout if self.base_model.config.classifier_dropout is not None else self.base_model.config.hidden_dropout_prob)\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.dense = nn.Linear(input_dim, input_dim)\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids=None, attention_mask=None, influencerAux=None, genderAux=None, mediaAux=None, labels=None):\n",
        "        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n",
        "        \n",
        "        if self.modelo == 'finiteautomata/beto-sentiment-analysis' or self.modelo == 'dccuchile/bert-base-spanish-wwm-cased':\n",
        "          outputs = torch.cat((outputs[1],influencerAux,genderAux,mediaAux), dim = -1)\n",
        "        else:\n",
        "          outputs = torch.cat((outputs[0][:,0,:],influencerAux,genderAux,mediaAux), dim = -1)\n",
        "          outputs = self.dropout(outputs)\n",
        "          outputs = self.dense(outputs)\n",
        "          outputs = torch.tanh(outputs)\n",
        "\n",
        "        outputs = self.dropout(outputs)\n",
        "        \n",
        "        logits = self.linear(outputs)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "          loss_fct = nn.CrossEntropyLoss()\n",
        "          loss = loss_fct(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def log(self, logs) -> None:\n",
        "        logs[\"learning_rate\"] = self._get_learning_rate()\n",
        "        super().log(logs)\n",
        "\n",
        "class Pruebas():\n",
        "\n",
        "    def __init__(self,dicObject):\n",
        "        self.dicObject = dicObject\n",
        "\n",
        "    def predictAndTrain(self,model,dicObject):\n",
        "\n",
        "        model= PosModelNoCapaOculta(4,model)\n",
        "        \n",
        "        dataset = load_dataset('csv', data_files={'train': 'TrainPreprocessCls.csv', 'test':'TestPreprocessCls.csv','validation': 'ValidationPreprocessCls.csv'})\n",
        "\n",
        "        #Cargamos los datos que necesitamos desde local:\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(dicObject['tokenizer'], model_max_length=dicObject['tam_Embeddings'])\n",
        "        #Preprocesamos todos los textos a la vez\n",
        "        tokenized_OffendES = dataset.map(self.preprocess_text, batched=True)\n",
        "        tokenized_OffendES['train'] = concatenate_datasets([tokenized_OffendES['train'], tokenized_OffendES['validation']])\n",
        "\n",
        "        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "         \n",
        "\n",
        "        training_args = TrainingArguments(         \n",
        "            output_dir=dicObject['directory'],                 \n",
        "            learning_rate=dicObject['learning_rate'],    \n",
        "            weight_decay=dicObject['weight_decay'],       \n",
        "            num_train_epochs=5,\n",
        "            per_device_train_batch_size=dicObject['batch_size'],      \n",
        "            per_device_eval_batch_size = dicObject['batch_size'],\n",
        "            load_best_model_at_end = True,\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            save_strategy=\"epoch\",\n",
        "            metric_for_best_model=\"eval_maf\",\n",
        "            disable_tqdm=True,\n",
        "            seed=seed\n",
        "    )     \n",
        "        trainer = MyTrainer(\n",
        "            model=model,\n",
        "            args=training_args,         \n",
        "            train_dataset=tokenized_OffendES[\"train\"],\n",
        "            eval_dataset=tokenized_OffendES[\"test\"],\n",
        "            tokenizer=self.tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=self.compute_metrics,\n",
        "            callbacks = [EarlyStoppingCallback(early_stopping_patience = 3)]  \n",
        "    )      \n",
        "    \n",
        "\n",
        "        trainer.train() \n",
        "        predictions = trainer.predict(tokenized_OffendES['test'])\n",
        "        predictions = tf.math.softmax(predictions.predictions, axis=-1)\n",
        "        tags=[]\n",
        "        for p in predictions:\n",
        "          max_value = max(p)\n",
        "          tags.append(np.where(p == max_value)[0][0])\n",
        "        df=pd.DataFrame()\n",
        "        df['label'] = tags\n",
        "        df.to_csv(str(dicObject['directory'].split('/')[1]+'tags.csv'), sep='\\t', index=False)\n",
        "        self.results(tags, tokenized_OffendES['test'][\"label\"], str(dicObject['directory'].split('/')[1]))\n",
        "\n",
        "    def results(self,pred_labels,true_labels, nameModel):\n",
        "        map = metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n",
        "        mar = metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n",
        "        maf1 = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='macro')\n",
        "        mip = metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n",
        "        mir = metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n",
        "        mif1 = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='micro')\n",
        "        wp = metrics.precision_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n",
        "        wr = metrics.recall_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n",
        "        wf1 = metrics.f1_score(y_true=true_labels, y_pred=pred_labels, average='weighted')\n",
        "        results = {'maf': maf1, 'map': map, 'mar': mar, 'mif': mif1, 'mip': mip, 'mir': mir, 'avgf': wf1, 'avgp': wp, 'avgr': wr}\n",
        "        df = pd.DataFrame(results, index=[0])\n",
        "        df.to_csv(str(nameModel+' '+'results.csv'), sep='\\t', index=False)\n",
        "\n",
        "    def preprocess_text(self, texts):\n",
        "        tokens = self.tokenizer(texts[\"comment\"],truncation=True, padding='max_length',max_length=self.dicObject['tam_Embeddings'])\n",
        "        tokens[\"influencerAux\"] = [eval(i) for i in texts[\"influencerAux\"]]\n",
        "        tokens[\"genderAux\"] = [eval(i) for i in texts[\"genderAux\"]]\n",
        "        tokens[\"mediaAux\"] = [eval(i) for i in texts[\"mediaAux\"]]\n",
        "        return tokens\n",
        "\n",
        "    def compute_metrics(self,p):    \n",
        "        pred, labels = p\n",
        "        pred = np.argmax(pred, axis=1)\n",
        "        \n",
        "        macp = metrics.precision_score(y_true=labels, y_pred=pred, average='macro')\n",
        "        mar = metrics.recall_score(y_true=labels, y_pred=pred, average='macro')\n",
        "        maf1 = metrics.f1_score(y_true=labels, y_pred=pred, average='macro')\n",
        "        mip = metrics.precision_score(y_true=labels, y_pred=pred, average='micro')\n",
        "        mir = metrics.recall_score(y_true=labels, y_pred=pred, average='micro')\n",
        "        mif1 = metrics.f1_score(y_true=labels, y_pred=pred, average='micro')\n",
        "        wp = metrics.precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
        "        wr = metrics.recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
        "        wf1 = metrics.f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
        "\n",
        "        return {'maf': maf1, 'map': macp, 'mar': mar, 'mif': mif1, 'mip': mip, 'mir': mir, 'avgf': wf1, 'avgp': wp, 'avgr': wr}\n",
        "\n",
        "    def entrenarModelo(self,model, dicParametersNoMetadata):\n",
        "        self.predictAndTrain(model, dicParametersNoMetadata)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  for i in dicParametersNoMetadata:\n",
        "    seed = 10\n",
        "    # python \n",
        "    random.seed(seed)\n",
        "    # pytorch\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "    # numpy \n",
        "    np.random.seed(seed)\n",
        "    p = Pruebas(dicParametersNoMetadata[i])\n",
        "    p.entrenarModelo(i,dicParametersNoMetadata[i])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}